{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Algorithm using MLlib\n",
    "## 1. Similarity\n",
    "### Pearson Correlation Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rho = \\frac{cov(X,Y)}{\\sigma_X\\sigma_Y} = \\frac{E[(X-\\mu_x)(Y-\\mu_Y)]}{\\sigma_X\\sigma_Y}$\n",
    "### Spearman Correlation Similarity\n",
    "$\\rho = 1- \\frac{6 \\sum{d_i^2}}{n(n^2-1)}$, where $d_i = x_i - y_i$.\n",
    "$x_i$ and $y_i$ are relative ranks of X and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SparseVector(4, {0: 1.0, 3: -2.0}),),\n",
       " (DenseVector([4.0, 5.0, 0.0, 3.0]),),\n",
       " (DenseVector([6.0, 7.0, 0.0, 8.0]),),\n",
       " (SparseVector(4, {0: 9.0, 3: 1.0}),)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Vectors.dense: traightforward\n",
    "# Vectors.sparse(number of parameters, location of non-zero indices, non-zero values)\n",
    "\n",
    "data = [(Vectors.sparse(4,[(0,1.0),(3,-2.0)]),),\n",
    "        (Vectors.dense([4.0,5.0,0.0,3.0]),),\n",
    "        (Vectors.dense([6.0,7.0,0.0,8.0]),),\n",
    "        (Vectors.sparse(4,[(0,9.0),(3,1.0)]),)]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data,['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df,'features').head()\n",
    "print('Pearson correlation matrix:\\n'+str(r1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r2 = Correlation.corr(df,'features','spearman').head()\n",
    "print('Spearman correlation matrix:\\n'+str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance Similarity\n",
    "$d(p,q) = d(q,p) = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + ... + (q_n - p_n)^2} =\\sqrt{\\sum_{i=1}^{n}{(q_i - p_i)^2}}$\n",
    "\n",
    "Similarity = $\\frac{1}{(1+d)}$\n",
    "\n",
    "### Cosine Similarity\n",
    "similarity = $cos(\\theta) = \\frac{A\\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i = 1}^n{A_i * B_i}}{\\sqrt{\\sum_{i = 1}^n{(A_i)^2}}* \\sqrt{\\sum_{i = 1}^n{(B_i)^2}}}$\n",
    "\n",
    "Cosine similarity and Pearson similarity get the same results if data are normalized.\n",
    "\n",
    "### Tanimoto(Jaccard) Coefficient Similarity\n",
    "$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "\n",
    "Tanimoto similarity is the same as Jaccard similarity, but Tanimoto distance is not the same as Jaccard distance.\n",
    "\n",
    "### Log-Likelihood Similarity\n",
    "$D = -2ln(\\frac{likelihood for null model}{likelihood for alternative model}) = -2ln(likelihood for null model)+2ln(likelihood for alternative model)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hypothesis Testing\n",
    "### Chi-Square Test for relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import ChiSquareTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, DenseVector([0.5, 10.0])),\n",
       " (0.0, DenseVector([1.5, 20.0])),\n",
       " (1.0, DenseVector([1.5, 30.0])),\n",
       " (0.0, DenseVector([3.5, 30.0])),\n",
       " (0.0, DenseVector([3.5, 40.0])),\n",
       " (1.0, DenseVector([3.5, 40.0]))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chi = [(0.0, Vectors.dense(0.5,10.0)),\n",
    "            (0.0, Vectors.dense(1.5,20.0)),\n",
    "            (1.0, Vectors.dense(1.5,30.0)),\n",
    "            (0.0, Vectors.dense(3.5,30.0)),\n",
    "            (0.0, Vectors.dense(3.5,40.0)),\n",
    "            (1.0, Vectors.dense(3.5,40.0))]\n",
    "data_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chi = spark.createDataFrame(data_chi, [\"label\",\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChiSqaureTest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b44377e8dc9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChiSqaureTest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_chi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pValues:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpValues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"degreesOfFreedom:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegreesOfFreedom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"statistics:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ChiSqaureTest' is not defined"
     ]
    }
   ],
   "source": [
    "r = ChiSqaureTest.test(df_chi, \"features\",\"label\").head()\n",
    "print(\"pValues:\"+str(r.pValues))\n",
    "print(\"degreesOfFreedom:\"+str(r.degreesOfFreedom))\n",
    "print(\"statistics:\"+str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clustering\n",
    "### K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_kmeans_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -831.2446592001428\n",
      "The upper bound on perplexity: 3.197094899166174\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[1, 3, 4]  |[0.10372354519692879, 0.10250759735058394, 0.09948183450247824]|\n",
      "|1    |[0, 5, 9]  |[0.10767378721176428, 0.09803424832133621, 0.09707077744224503]|\n",
      "|2    |[5, 10, 9] |[0.09819703532331234, 0.09813708797456176, 0.09566060479222838]|\n",
      "|3    |[5, 10, 2] |[0.10433364722402798, 0.10204517073508769, 0.09789654677807182]|\n",
      "|4    |[5, 6, 8]  |[0.16500150710907283, 0.0975079662858406, 0.09480370668847367] |\n",
      "|5    |[2, 1, 5]  |[0.101818122360764, 0.09675771244912808, 0.09604418555681972]  |\n",
      "|6    |[6, 10, 1] |[0.1773541132783834, 0.17521379652075258, 0.14105657947030356] |\n",
      "|7    |[8, 3, 5]  |[0.10453790009606602, 0.09705023349094455, 0.09687785324125661]|\n",
      "|8    |[2, 1, 5]  |[0.11204345094026681, 0.09725990306627529, 0.09706870509125809]|\n",
      "|9    |[9, 1, 8]  |[0.1044693724664232, 0.09727486280488697, 0.09680749028744454] |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                     |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.00473101766443592,0.004731027789917107,0.004731023678058601,0.004731055444332503,0.5947632956783104,0.004731090093380107,0.36738838951273883,0.004730983716571899,0.004731142686907307,0.004730973735347003]       |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.00789478717318037,0.007893545805620045,0.007893666451644273,0.007893570539017883,0.008161377288708726,0.007893685250151229,0.9286882802784081,0.007893693853396243,0.0078937870974899,0.007893606262383102]        |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004112865275321488,0.004112947779276705,0.004112897820942627,0.004112920147968843,0.15323272673469826,0.0041129091332646845,0.8138640197606404,0.00411290717860981,0.004112941014730115,0.0041128651545469614]     |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.003637639513828591,0.003637632785457303,0.0036376007703898825,0.0036376078503624213,0.003760977177093169,0.003637609280309114,0.9671381620930227,0.003637598340568972,0.0036375716073450072,0.0036376005816227933] |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.003941467889224904,0.003941282441097653,0.003941297115485027,0.003941334633910658,0.004075015125432305,0.003941341338182526,0.9643943305343294,0.003941316534590175,0.003941332397992698,0.003941281989754638]     |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.0036378353884973865,0.0036378650898114743,0.003637850010842854,0.0036378420139984947,0.3151377216549431,0.003637854895058028,0.6557594967230563,0.0036378535692522026,0.0036378558392247493,0.0036378248153153656] |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.0037834343497652363,0.003783317993086091,0.003783293873493633,0.0037833022167483017,0.003911615150303053,0.003783294177603972,0.9658218463510504,0.00378329414968081,0.0037832642218308433,0.0037833375164377015]  |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004300271930016199,0.0043001638557219534,0.004300193864636715,0.004300259515049037,0.004446332682665388,0.0043002421668956944,0.9611519005195333,0.004300224303629371,0.0043002442153821825,0.004300166946470344]  |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004300406907711561,0.004300394982701774,0.004300419811439464,0.004300388899361206,0.00444672002725732,0.004300439557126108,0.9611499442380618,0.004300433123758418,0.004300493231675766,0.004300359220906701]      |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.0032609809790293503,0.0032609729606075315,0.0032609465420606744,0.00326094868995837,0.003371596855498729,0.0032609696868800982,0.9705406944844575,0.0032609473178698595,0.0032609823200380416,0.003260960163599972]|\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004113010881988357,0.00411298633568753,0.004113002261080766,0.004113058587155073,0.004252971463824229,0.004113036173967469,0.9628428844543616,0.004113029403078295,0.00411304567878758,0.004112974760068961]       |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.004731066015005334,0.004731123268051476,0.004731100685508,0.004731023573749889,0.6339851355465903,0.004731092896353718,0.3281662325106996,0.004731075369492518,0.00473111538872182,0.004731034745827168]           |\n",
      "+-----+---------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_lda_libsvm_data.txt\")\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bisecting k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 0.11999999999994547\n",
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(2).setSeed(1)\n",
    "model = bkm.fit(dataset)\n",
    "\n",
    "# Evaluate clustering.\n",
    "cost = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "\n",
    "# Shows the result.\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification\n",
    "### Binomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[244,263,272,300,301,328,350,351,378,379,405,406,407,428,433,434,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.353983524188197e-05,-9.102738505589466e-05,-0.00019467430546904298,-0.00020300642473486668,-3.1476183314863995e-05,-6.842977602660743e-05,1.5883626898239883e-05,1.4023497091372047e-05,0.00035432047524968605,0.00011443272898171087,0.00010016712383666666,0.0006014109303795481,0.0002840248179122762,-0.00011541084736508837,0.000385996886312906,0.000635019557424107,-0.00011506412384575676,-0.00015271865864986808,0.0002804933808994214,0.0006070117471191634,-0.0002008459663247437,-0.0001421075579290126,0.0002739010341160883,0.00027730456244968115,-9.838027027269332e-05,-0.0003808522443517704,-0.00025315198008555033,0.00027747714770754307,-0.0002443619763919199,-0.0015394744687597765,-0.00023073328411331293])\n",
      "Intercept: 0.22456315961250325\n",
      "Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,244) 0.0\n",
      "(0,263) 0.0001\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0\n",
      "(0,351) -0.0\n",
      "(0,378) -0.0\n",
      "(0,379) -0.0\n",
      "(0,405) -0.0\n",
      "(0,406) -0.0006\n",
      "(0,407) -0.0001\n",
      "(0,428) 0.0001\n",
      "(0,433) -0.0\n",
      "(0,434) -0.0007\n",
      "(0,455) 0.0001\n",
      "(0,456) 0.0001\n",
      "..\n",
      "..\n",
      "Multinomial intercepts: [-0.12065879445860686,0.12065879445860686]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "3 X 4 CSRMatrix\n",
      "(0,3) 0.3176\n",
      "(1,2) -0.7804\n",
      "(1,3) -0.377\n",
      "Intercept: [0.05165231659832854,-0.12391224990853622,0.07225993331020768]\n",
      "objectiveHistory:\n",
      "1.098612288668108\n",
      "1.087602085441699\n",
      "1.0341156572156232\n",
      "1.0289859520256006\n",
      "1.0300389657358995\n",
      "1.0239965158223991\n",
      "1.0236097451839508\n",
      "1.0231082121970012\n",
      "1.023022220302788\n",
      "1.0230018151780262\n",
      "1.0229963739557606\n",
      "False positive rate by label:\n",
      "label 0: 0.22\n",
      "label 1: 0.05\n",
      "label 2: 0.0\n",
      "True positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      "label 2: 0.46\n",
      "Precision by label:\n",
      "label 0: 0.6944444444444444\n",
      "label 1: 0.9090909090909091\n",
      "label 2: 1.0\n",
      "Recall by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      "label 2: 0.46\n",
      "F-measure by label:\n",
      "label 0: 0.819672131147541\n",
      "label 1: 0.9523809523809523\n",
      "label 2: 0.6301369863013699\n",
      "Accuracy: 0.82\n",
      "FPR: 0.09\n",
      "TPR: 0.82\n",
      "F-measure: 0.8007300232766211\n",
      "Precision: 0.8678451178451179\n",
      "Recall: 0.82\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark \\\n",
    "    .read \\\n",
    "    .format(\"libsvm\") \\\n",
    "    .load(\"/usr/local/spark/data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for multinomial logistic regression\n",
    "print(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(lrModel.interceptVector))\n",
    "\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[100,101,102...|\n",
      "|       0.0|         1.0|(692,[121,122,123...|\n",
      "|       0.0|         1.0|(692,[122,123,148...|\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.25 \n",
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_445282127120f667a6cf) of depth 2 with 5 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[100,101,102...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.222222\n",
      "GBTClassificationModel (uid=GBTClassifier_4881ba9d068ed81e5ae4) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = model.stages[2]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.9019607843137255\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/usr/local/spark/data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [4, 5, 4, 3]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(train)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "result = model.transform(test)\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005170630317473439,-0.0001172288654973735,-8.882754836918948e-05,8.522360710187464e-05,0.0,0.0,-1.3436361263314267e-05,0.0003729569801338091,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0008888949552633658,0.00029864059761812683,0.0003793378816193159,-0.0001762328898254081,0.0,1.5028489269747836e-06,1.8056041144946687e-06,1.8028763260398597e-06,-3.3843713506473646e-06,-4.041580184807502e-06,2.0965017727015125e-06,8.536111642989494e-05,0.00022064177429604464,0.00021677599940575452,-0.0005472401396558763,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.000921415502407147,0.00031351066886882195,0.0002481984318412822,0.0,-4.147738197636148e-05,-3.6832150384497175e-05,0.0,-3.9652366184583814e-06,-5.1569169804965594e-05,-6.624697287084958e-05,-2.182148650424713e-05,1.163442969067449e-05,-1.1535211416971104e-06,3.8138960488857075e-05,1.5823711634321492e-06,-4.784013432336632e-05,-9.386493224111833e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.00043174897827077767,0.00017055492867397665,0.0,-2.7978204136148868e-05,-5.88745220385208e-05,-4.1858794529775e-05,-3.740692964881002e-05,-3.9787939304887e-05,-5.545881895011037e-05,-4.505015598421474e-05,-3.214002494749943e-06,-1.6561868808274739e-06,-4.416063987619447e-06,-7.9986183315327e-06,-4.729962112535003e-05,-2.516595625914463e-05,-3.6407809279248066e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00024719098130614967,0.0,-3.270637431382939e-05,-5.5703407875748054e-05,-5.2336892125702286e-05,-7.829604482365818e-05,-7.60385448387619e-05,-8.371051301348216e-05,-1.8669558753795108e-05,0.0,1.2045309486213725e-05,-2.3374084977016397e-05,-1.0788641688879534e-05,-5.5731194431606874e-05,-7.952979033591137e-05,-1.4529196775456057e-05,8.737948348132623e-06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0012589360772978808,-0.0001816228630214369,-0.00010650711664557365,-6.040355527710781e-05,-4.856392973921569e-05,-8.973895954652451e-05,-8.78131677062384e-05,-5.68487774673792e-05,-3.780926734276347e-05,1.3834897036553787e-05,7.585485129441565e-05,5.5017411816753975e-05,-1.5430755398169695e-05,-1.834928703625931e-05,-0.00010354008265646844,-0.00013527847721351194,-0.00011245007647684532,-2.9373916056750564e-05,-7.311217847336934e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0002858228613863785,-0.00012998173971449976,-0.0001478408021316135,-8.203374605865772e-05,-6.556685320008032e-05,-5.6392660386580244e-05,-6.995571627330911e-05,-4.664348159856693e-05,-2.3026593698824318e-05,7.398833979172035e-05,0.00014817176130099997,0.00010938317435545486,7.940425167011364e-05,-6.743294804348106e-07,-0.00012623302721464762,-0.00019110387355357616,-0.00018611622108961136,-0.00012776766254736952,-8.935302806524433e-05,-1.239417230441996e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0002829530831354112,-0.00013912189600461263,-0.00012593136464577562,-5.964745187930992e-05,-5.360328152341982e-05,-0.00010517880662090183,-0.00013856124131005022,-7.181032974125911e-05,2.3249038865093483e-06,0.0001566964269571967,0.00023261206954040812,0.00017261638232256968,0.00013857530960270466,-1.396299028868332e-05,-0.00015765773982418597,-0.00020728798812007546,-0.00019106441272002828,-0.00012744834161431415,-0.00012755611630280015,-5.1885591560478935e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.000159081567023441,-0.0001216531230287931,-5.623851079809818e-05,-3.877987126382982e-05,-7.550900509956966e-05,-0.00010703140005463545,-0.00014720428138106226,-8.781423374509368e-05,7.941655609421792e-05,0.00023206354986219992,0.00027506982343672394,0.0002546722233188043,0.0001810821666388498,-1.3069916689929984e-05,-0.0001842374220886751,-0.0001977540482445517,-0.00017722074063670741,-0.0001487987014723575,-0.00011879021431288621,-9.755283887790393e-05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0001302740311359312,-5.3683030235535024e-05,-1.7631200013656873e-05,-7.846611034608254e-05,-0.000122100767283256,-0.00017281968533449702,-0.00015592346128894157,-5.239579492910452e-05,0.0001680719343542442,0.00028930086786548053,0.0003629921493231646,0.0002958223512266975,0.00021770466955449064,-6.40884808188951e-05,-0.00019058225556007997,-0.00020425138564600712,-0.0001711994903702119,-0.00013853486798341369,-0.00013018592950855062,-0.00011887779512760102,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-7.021411112285498e-05,-1.694500843168125e-05,-7.189722824172193e-05,-0.00014560828004346436,-0.00014935497340563198,-0.00019496419340776972,-0.00017383743417254187,-3.3438825792010694e-05,0.0002866538327947017,0.00029812321570739803,0.000377250607691119,0.0003211702827486386,0.0002577995115175486,-0.00016627385656703205,-0.00018037105851523224,-0.00020419356344211325,-0.00017962237203420184,-0.00013726488083579862,-0.00013461014473741762,-0.00012264216469164138,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0015239752514658556,-5.472330865993813e-05,-9.65684394936216e-05,-0.00013424729853486994,-0.00014727467799568,-0.0001616270978824712,-0.00018458259010029364,-0.00019699647135089726,0.00013085261294290817,0.0002943178857107149,0.0003097773692834126,0.0004112834769312103,0.00034113620757035025,0.00016529945924367265,-0.00021065410862650534,-0.0001883924081539624,-0.0001979586414569358,-0.0001762131187223702,-0.0001272343622678854,-0.00012708161719220297,-0.00014812221011889967,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.001140680600536578,-0.0001323467421269896,-0.00012904607854274846,-0.00014104748544921958,-0.00015194605434027872,-0.00021104539389774283,-0.00017911827582001795,-0.00018952948277194435,0.00021767571552539842,0.00030201791656326465,0.0004002863274397723,0.00040322806756364006,0.0004118077382608461,3.7917405252859545e-06,-0.00019886290660234838,-0.00019547443112937263,-0.00019857348218680872,-0.00013336892200703206,-0.00012830129292910815,-0.00011855916317355505,-0.0001765597203760205,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0010938769592297973,-0.00012785475305234688,-0.00013424699777466666,-0.0001505200652479287,-0.00019333287822872713,-0.00020385160086594937,-0.00017422470698847553,4.63598443910652e-05,0.00020617623087127652,0.0002862882891134514,0.0004074830988361515,0.0003726357785147985,0.0003507520190729629,-0.0001516485494364312,-0.00017053751921469217,-0.00019638964654350848,-0.00019962586265806435,-0.00013612312664311173,-0.0001218285533892454,-0.00011166712081624676,-0.0001377283888177579,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0003044386260118809,-0.0001240836643202059,-0.0001335317492716633,-0.00015783442604618277,-0.00019168434243384107,-0.00018710322733892716,-0.00011283989231463139,0.00011136504453105364,0.00018707244892705632,0.00028654279528966305,0.00040032117544983536,0.0003169637536305377,0.00020158994278679014,-0.00013139392844616033,-0.00015181070482383948,-0.0001825431845981843,-0.0001602539928567571,-0.00013230404795396355,-0.00011669138691257469,-0.00010532154964150405,-0.00013709037042366007,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00040287410145021705,-0.00013563987950912995,-0.00013225887084018914,-0.00016523502389794188,-0.00020175074284706945,-0.0001572459106394481,2.577536501278673e-06,0.0001312463663419457,0.00020707422291927531,0.00039081065544314936,0.00033487058329898135,0.00025790441367156086,2.6881819648016494e-05,-0.0001511383586714907,-0.0001605428139328567,-0.00017267287462873575,-0.00011938943768052963,-0.00010505245038633314,-0.00011109385509034013,-0.00013469914274864725,-0.00020735223736035555,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005034374233912422,-0.00015961213688405883,-0.0001274222123810994,-0.0001582821104884909,-0.00021301220616286252,-0.00012933366375029613,1.6802673102179614e-05,0.00011020918082727098,0.00021160795272688753,0.00034873421050827716,0.00026487211944380384,0.0001151606835026639,-5.4682731396851946e-05,-0.00013632001630934325,-0.00014340405857651405,-0.0001248695773821634,-8.462873247977974e-05,-9.580708414770257e-05,-0.00010749166605399431,-0.00014618038459197777,-0.00037556446296204636,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0005124342611878493,-0.00020369734099093433,-0.00013626985098328694,-0.00013313768183302705,-0.0001871555537819396,-0.0001188817315789655,-1.8774817595622694e-05,5.7108412194993384e-05,0.00012728161056121406,0.00019021458214915667,0.00012177397895874969,-1.2461153574281128e-05,-7.553961810487739e-05,-0.00010242174559410404,-4.44873554195981e-05,-9.058561577961895e-05,-6.837347198855518e-05,-8.084409304255458e-05,-0.00013316868299585082,-0.00020335916397646626,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.0003966510928472775,-0.00013738983629066386,-3.7971221409699866e-05,-6.431763035574533e-05,-0.00011857739882295322,-9.359520863114822e-05,-5.0878371516215046e-05,-8.269367595092908e-08,0.0,1.3434539131099211e-05,-1.9601690213728576e-06,-2.8527045990494954e-05,-7.410332699310603e-05,-7.132130570080122e-05,-4.9780961185536e-05,-6.641505361384578e-05,-6.962005514093816e-05,-7.752898158331023e-05,-0.00017393609499225025,-0.0012529479255443958,0.0,0.0,0.00020682521269893754,0.0,0.0,0.0,0.0,0.0,-0.00046702467383631055,-0.00010318036388792008,1.2004408785841247e-05,0.0,-2.5158639357650687e-05,-1.2095240910793449e-05,-5.19052816902203e-06,-4.916790639558058e-06,-8.48395853563783e-06,-9.362757097074547e-06,-2.0959335712838412e-05,-4.7790091043859085e-05,-7.92797600958695e-05,-4.462687041778011e-05,-4.182992428577707e-05,-3.7547996285851254e-05,-4.52754480225615e-05,-1.8553562561513456e-05,-0.00024763037962085644,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00034886180455242474,-5.687523659359091e-06,7.380040279654313e-05,4.395860636703821e-05,7.145198242379862e-05,6.181248343370637e-06,0.0,-6.0855538083486296e-05,-4.8563908323274725e-05,-4.117920588930435e-05,-4.359283623112936e-05,-6.608754161500044e-05,-5.443032251266018e-05,-2.7782637880987207e-05,0.0,0.0,0.0002879461393464088,-0.0028955529777851255,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.00012312114837837392,-1.9526747917254753e-05,-1.6999506829961688e-05,5.4835294148085086e-05,1.523441632762399e-05,-5.8365604525328614e-05,-0.00012378194216521848,-0.00011750704953254656,-6.19711523061306e-05,-5.042009645812091e-05,-0.00014055260223565886,-0.0001410330942465528,-0.00019272308238929396,-0.0004802489964676616]\n",
      "Intercept: 0.012911305214513969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lsvcModel = lsvc.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear SVC\n",
    "print(\"Coefficients: \" + str(lsvcModel.coefficients))\n",
    "print(\"Intercept: \" + str(lsvcModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-vs-Rest classifier (a.k.a. One-vs-All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.037037\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# load data file.\n",
    "inputData = spark.read.format(\"libsvm\") \\\n",
    "    .load(\"/usr/local/spark/data/mllib/sample_multiclass_classification_data.txt\")\n",
    "\n",
    "# generate the train/test split.\n",
    "(train, test) = inputData.randomSplit([0.8, 0.2])\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(train)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(test)\n",
    "\n",
    "# obtain evaluator.\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|label|            features|       rawPrediction|probability|prediction|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "|  0.0|(692,[95,96,97,12...|[-174115.98587057...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[98,99,100,1...|[-178402.52307196...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[100,101,102...|[-100905.88974016...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[-244784.29791241...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[-196900.88506109...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-238164.45338794...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[-184206.87833381...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[-214174.52863813...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[-182844.62193963...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[128,129,130...|[-246557.10990301...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[-208282.08496711...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[-243457.69885665...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[-260933.50931276...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[-220274.72552901...|  [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[181,182,183...|[-154830.07125175...|  [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[99,100,101,...|[-145978.24563975...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[100,101,102...|[-147916.32657832...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[123,124,125...|[-139663.27471685...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-129013.44238751...|  [0.0,1.0]|       1.0|\n",
      "|  1.0|(692,[125,126,127...|[-81829.799906049...|  [0.0,1.0]|       1.0|\n",
      "+-----+--------------------+--------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\") \\\n",
    "    .load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.show()\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression\n",
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.32292516677405936,-0.3438548034562218,1.9156017023458414,0.05288058680386263,0.765962720459771,0.0,-0.15105392669186682,-0.21587930360904642,0.22025369188813426]\n",
      "Intercept: 0.1598936844239736\n",
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053554|\n",
      "|  -5.204019455758823|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719486|\n",
      "|  -10.00431602969873|\n",
      "|   2.062397807050484|\n",
      "|  3.1117508432954772|\n",
      "| -15.893608229419382|\n",
      "|  -5.036284254673026|\n",
      "|   6.483215876994333|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "| -2.0049838218725005|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/usr/local/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.010541828081257216,0.8003253100560949,-0.7845165541420371,2.3679887171421914,0.5010002089857577,1.1222351159753026,-0.2926824398623296,-0.49837174323213035,-0.6035797180675657,0.6725550067187461]\n",
      "Intercept: 0.14592176145232041\n",
      "Coefficient Standard Errors: [0.7950428434287478, 0.8049713176546897, 0.7975916824772489, 0.8312649247659919, 0.7945436200517938, 0.8118992572197593, 0.7919506385542777, 0.7973378214726764, 0.8300714999626418, 0.7771333489686802, 0.463930109648428]\n",
      "T Values: [0.013259446542269243, 0.9942283563442594, -0.9836067393599172, 2.848657084633759, 0.6305509179635714, 1.382234441029355, -0.3695715687490668, -0.6250446546128238, -0.7271418403049983, 0.8654306337661122, 0.31453393176593286]\n",
      "P Values: [0.989426199114056, 0.32060241580811044, 0.3257943227369877, 0.004575078538306521, 0.5286281628105467, 0.16752945248679119, 0.7118614002322872, 0.5322327097421431, 0.467486325282384, 0.3872259825794293, 0.753249430501097]\n",
      "Dispersion: 105.60988356821714\n",
      "Null Deviance: 53229.3654338832\n",
      "Residual Degree Of Freedom Null: 500\n",
      "Deviance: 51748.8429484264\n",
      "Residual Degree Of Freedom: 490\n",
      "AIC: 3769.1895871765314\n",
      "Deviance Residuals: \n",
      "+-------------------+\n",
      "|  devianceResiduals|\n",
      "+-------------------+\n",
      "|-10.974359174246889|\n",
      "| 0.8872320138420559|\n",
      "| -4.596541837478908|\n",
      "|-20.411667435019638|\n",
      "|-10.270419345342642|\n",
      "|-6.0156058956799905|\n",
      "|-10.663939415849267|\n",
      "| 2.1153960525024713|\n",
      "| 3.9807132379137675|\n",
      "|-17.225218272069533|\n",
      "| -4.611647633532147|\n",
      "| 6.4176669407698546|\n",
      "| 11.407137945300537|\n",
      "| -20.70176540467664|\n",
      "| -2.683748540510967|\n",
      "|-16.755494794232536|\n",
      "|  8.154668342638725|\n",
      "|-1.4355057987358848|\n",
      "|-0.6435058688185704|\n",
      "|  -1.13802589316832|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "# Load training data\n",
    "dataset = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/usr/local/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "\n",
    "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
    "\n",
    "# Fit the model\n",
    "model = glr.fit(dataset)\n",
    "\n",
    "# Print the coefficients and intercept for generalized linear regression model\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[95,96,97,12...|\n",
      "|       0.0|  0.0|(692,[100,101,102...|\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[122,123,148...|\n",
      "|       0.0|  0.0|(692,[125,126,127...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.185695\n",
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_40518e6cebe82e0c28d0) of depth 2 with 5 nodes\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load the data stored in LIBSVM format as a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "treeModel = model.stages[1]\n",
    "# summary only\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[95,96,97,12...|\n",
      "|       0.0|  0.0|(692,[122,123,148...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.137773\n",
      "RandomForestRegressionModel (uid=RandomForestRegressor_414e918847aef4fffc48) with 20 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       1.0|  0.0|(692,[98,99,100,1...|\n",
      "|       0.0|  0.0|(692,[122,123,124...|\n",
      "|       0.0|  0.0|(692,[122,123,148...|\n",
      "|       0.0|  0.0|(692,[123,124,125...|\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 0.254\n",
      "GBTRegressionModel (uid=GBTRegressor_46119d03cf68f8d59022) with 10 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Load and parse the data file, converting it to a DataFrame.\n",
    "data = spark.read.format(\"libsvm\").load(\"/usr/local/spark/data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a GBT model.\n",
    "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.49631114666506754,0.19844437699934114]\n",
      "Intercept: 2.6380946151040043\n",
      "Scale: 1.5472345574364688\n",
      "+-----+------+--------------+------------------+--------------------------------------+\n",
      "|label|censor|features      |prediction        |quantiles                             |\n",
      "+-----+------+--------------+------------------+--------------------------------------+\n",
      "|1.218|1.0   |[1.56,-0.605] |5.718979487634966 |[1.1603238947151588,4.995456010274735]|\n",
      "|2.949|0.0   |[0.346,2.158] |18.076521181495643|[3.667545845471804,15.789611866277896]|\n",
      "|3.627|0.0   |[1.38,0.231]  |7.381861804239103 |[1.497706130519085,6.447962612338967] |\n",
      "|0.273|1.0   |[0.52,1.151]  |13.577612501425385|[2.7547621481507076,11.85987222406979]|\n",
      "|4.199|0.0   |[0.795,-0.226]|9.013097744073846 |[1.8286676321297732,7.872826505878384]|\n",
      "+-----+------+--------------+------------------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import AFTSurvivalRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "training = spark.createDataFrame([\n",
    "    (1.218, 1.0, Vectors.dense(1.560, -0.605)),\n",
    "    (2.949, 0.0, Vectors.dense(0.346, 2.158)),\n",
    "    (3.627, 0.0, Vectors.dense(1.380, 0.231)),\n",
    "    (0.273, 1.0, Vectors.dense(0.520, 1.151)),\n",
    "    (4.199, 0.0, Vectors.dense(0.795, -0.226))], [\"label\", \"censor\", \"features\"])\n",
    "quantileProbabilities = [0.3, 0.6]\n",
    "aft = AFTSurvivalRegression(quantileProbabilities=quantileProbabilities,\n",
    "                            quantilesCol=\"quantiles\")\n",
    "\n",
    "model = aft.fit(training)\n",
    "\n",
    "# Print the coefficients, intercept and scale parameter for AFT survival regression\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "print(\"Scale: \" + str(model.scale))\n",
    "model.transform(training).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
